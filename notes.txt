Machine Learning with Python
11.16.19
-------------------------------------
Q?: DO I need to download the course materials or do I have lifetime access?
Q?: Can you donwload the jupyter notebook files?

-------------------------------------
############################
Week One:
What is Machine Learning
############################

Welcome:
Learn how ML is used in key fields.
Predict whethter a cell is benign or malignant
Learn the value of decision trees.
Bankers on loan decisions. Segmentation of data.
Helps websites make recommendations.
scikit learn to estimate co2 emissions.
Predict customer churn.

Can star tthe lab environment in the browser (through Jupyter notebooks).
New Skills to be learned:
Regression
Classification
Clustering
Scikit Learn
Scipy

New Projects:
Cancer Detection
Predicting Economic Trends
Predicting customer Churn
Recommendation engines


Introduction to Machine Learning:

High level intro to ML.
A cell has many characteristics that we can review.
ML helps with predictions. Analysis of data by evaluating characteristics. Clean data, select proper algorithm, and train the model.
Once trained iteratively (i.e. going through each record), it can be used to predict the new data. This is ML.

Def Machine Learning:
Is the subfield of computer science that gives computers the ability to learn without being explicitly programmed.

Ex. Suppose we want create a program to recognize images of cats and dogs.
The first thing to do is to break down the images into feature sets. For example, does the image show eyes, and what is the size. Legs, tails, etc are all attributes. Convert to a vector of features.
ML helped change the difficulty of this. It allows us to look at all feature sets and the corresponding animals. It learns the pattern of each animal.
It detects without being explicitly programmed to do so.

Examples of ML:
Netflix and Amazon. ML produces suggestions. Based on the knowledge and finding patterns within the data set.

Machine Learning Techniques:
Regression/ Estimation
    - Predicting a continuous value.
    - Ex. A house. The CO2 emissions
Classification
    - Predicting the item class/ category of a case.
    - Ex. A cell is benign or malignant. Customer Churn
Clustering
    - Finding the structure of data; summarization
    - Ex. Find similiarities or create segmentation
Associations
    - Associating frequent co-occurring items/ events
    - Ex. Grocery items that are bought together.
Anomaly detection:
    - Discovering abnormal and unusual cases
    - Ex. Credit card fraud detection
Sequence mining:
    - Predicting next events
    - Ex. click-stream (Markov Model, HMM)
Dimension Reduction:
    - Reducing the size of data (PCA)
    - Ex.
Recommendation Systems
    - Recommending items
    - Ex. Associates ppls prefrence with others with similar tastes and recommends new items to them.


Difference between aritificial intelligence, machine learning, and deep learning
AI components: Tries to make computers intelligent by mimicing the coginitive functions of humans
    - Computer Vision
    - Language Processing
    - Creativity
Machine Learning: IS the branch of AI that covers the statistical part of artificial intelligence. It teaches the computer to solve problems y going through tons of data, learning from them, and then using that experience to make predictions and solve the problems with the new data.
    - Classification
    - Clustering
    - Neural Networks
Revolution in ML:
    - Deep learning: A special field of ML. Where computers can actually learn and make intelligent decisions on their own. Deeper level of automation.

Machine Learning Applications
Machine Learning Algorithms

----------------------------
Python for Machine Learning:
----------------------------

Python Libraries for ML:
NumPy: Math library to work with n-dimensional arrays.
Computation is efficient, much better than vanilla python.

SciPy: Collection of numerical algorithms. Good library for scientific computation.

Matplotlib: Plotting package.

These three pkgs are discussed in the Data Analysis with Python course. Recommended to take these first.

pandas: A high level lib that provides high performance work.

scikit learn: Collection of Algorithms and tools for ML.
- Free ML lib for Python.
- Has most of the Classification, Regression, and Clustering algorithms.
- Works with NumPy and SciPy
- Great documenation.
- Easy to implement ML models.

sci-kit learn functions:
Will go over these throughout the course.
ML algorithms benefit from standardization of datasets.
sci-kit learn makes all of this very easy.
training set and test sets.
test set to run prediction (likely after trained).

All of this would be difficult with just NumPy, especially further if it was pure Python.

----------------------------------------
Supervised versus Unsupervised Learning:
----------------------------------------

What is supervised learning?
To observe or direct the ML model. Be able to produce classification regions.
How do we do this?
By teaching the model. We load the model with knowledge and then it can use this to predict future instances.

Teaching the model with labeled data:
Note that it is labelled data.
The header row labels are called attributes. Columns are called features and they include the data.
A row would have all the attributes and it is also known as an observation.
Two kinds of data:
    1. Numeric
    2. Categorical: non-numeric; has characters like "beign"

There are two types of Supervised Learning techniques:
Classification and Regression

1. Classification:
Is the process of predicting discrete class labels or categories.
2. Regression:
Is the process of predicting a continuous value versus Categorical in classification. (Q?: So more numerical?)

For example, if you have a dataset of engines with their attributes, you can use this to regressively calculate a new engines co2 levels, provided you are given some attributes that are similar to the dataset.

What is unsupervised learning?
We do not supervise the model and let it work on its own to discover information that may be hidden to us.
It works on the dataset and draws conclusions based on the unlabelled data.
Generally speaking, the algorithms are more difficult.

Unsupervised techniques:
    - Dimension reduction
    - Density estimation
    - Market Basket analysis
    - Clustering

Dimensionality reduction (and/ or feature selection):
reduces redundant features to make classification easier.
Market Basket Analysis:
is a modelling technique based on the thery that if you buy one group of items you are more likely to buy from another group.
Density Estimation:
Simple concept used to explore the data to find some structure within it.
Clustering: one of the most popular in UnSupervised Learning
Is grouping of data points or objects that are somehow similar by: discovering structure, summarization, and anomaly detection.
Many applications, like Bank segmenting customers, helping an individual organize music. The three above are the most common or general use of this technique.

Supervised versus unsupervised learning:
classification:
labeled data

regression:
predicts trends using previous labeled data

has more evaluation methods

controlled env

Unsupervised:
clustering:
Finds patterns and groupings from unlabelled data

Has fewer evaluation methods than supervised learning

Less controlled environment.

----------
Quiz:
----------

###################################
Week Two: Linear Regression
###################################
Due Dec. 2
Estimated time: 5 hrs.
---------------------------
Introduction to Regression:
---------------------------

What is regression?
Given a dataset of cars, for example, can we predict the co2emissions of a car by looking at its engine size and number cylinders, given that these among other things are the attributes of each car in the DS.

Regression is the process of predicting a continuous value, and yes, we can try to predict the co2 emissions using other variables here.

There are two types of variables in regression:
    - Dependent variable: State/ target/ goal that we are trying to predict.
    - One or more independent variables: also known as explanatory variables, can be seen as the causes of those states. So in the case of the co2, the engine size and cylinders would be the independent vars.

By convention, independent vars are shown as X var, and the dependent Y. Y is related to a function of X. Y is to be solved.
The dependent variable must be continuous and cannot be a discrete value. The independent variables can be measured on either a continuous or categorical scale.

Q?: Why the difference?
A!: Discrete referring to something that must exist in the set, whereas continues has an infinite number of possibilities.

What is a regression model?
Use the historical data, using one or more of it's features (column data), and from that data make a model.
We use a regression to build the regression estimation model, which attempts to fit a line to the data.

There are two types of regression models:
Simple Regression : When one independent variable (X) is used to estimate one dependent variable (Y).
    - Can be linear or not
    - Simple Linear Regression
    - Simple Non-linear Regression
Multiple Regression: When more than one independent variable is used to determine/ estimate the dependent variable.
    - Ex. Predicting co2 emissions with both engine size and number of cylinders.
    - Multiple Linear Regression
    - Multiple Non-linear Regression

Applications of Regression
We use regression when we want to estimate a continuous value.
Sales forecasting: Predict total yearly sales using linear regression.
Psychology: Individual satisfaction based on socio economic status
Price estimate: for a house
Employment income: based on age, gender, etc.
Quiz: Rainfall

Regression algorithms: There are many and each has specific application in which they are best suited.
    - Ordinal regression
    - Poisson regression
    - Fast forest quantile regression
    - Linear, Polynomial, Lasoo, Stepwise, Ridge regression
    - Bayesian linear regression
    - Neural network regression
    - Decision forest regression
    - Boosted decision tree regression
    - KNN (K-nearest neighbors)


-------------------------
Simple Linear Regression:
-------------------------

We can predict the value of a dependent variable using a single independent variable. This is simple linear regression- by using 1 independent(X)  and 1 dependent variable (Y).
The key point is that our dependent variable must be continous and non-discrete. However, the independent variables can either be on a continuous or categorical scale.
# Note: It seems that the distinction is that the independent variable must be numeric and non-categorical. But does this mean we can never estimate the category of something? Linear regression is pretty simple, but in mathematical terms.

There are two types of regression models/ topology:
Simple Linear regression: When one independent variable is used to represent one dependent variable.
Multiple Linear Regression: More than one independent variable used to determine the dependent variable.

A scatter plot can easily show that changes in one variable explain or possibly cause (important distinction between cause explain) changes or the state of another variable. It can also show that the relationship between the two is linear.
The whole point of this type of work is to fit a line between the data, if it is in fact linear.

Linear regression model representation:
We now talk about the "fitting line" actually means. We are going to find y based on x.
Traditionally, this is shown as a polynomial (consisting of more than one term a la x & y):
yhat = theta_zero + theta_one*x1

In this, y hat is the dependent/ predicted variable and x1 is the independent variable.
theta_zero and theta_one are the parameters of the line that we must adjust.
theta_one is the slope and theta_zero is the intercept. These are also referred to as the coefficients of the linear equation.

The question then becomes how do you draw a line between the points and also how do you determine what line fits best?
Linear regression estimates the coefficients of the line, which means we must calculate these things.

How to find the best fit?
Assume that we have found the best fit line already. Let's go through all of the points and see how well they fit with the line.
Basically, we should be able to take an x1 (engine size in these examples) and see that the predicted value is close to the actual value.
Said another way, if we use our polynomial with known parameters, we hope that that this predicted value, y_hat, is close to the actual value y.

Q?: Is y_hat used only for predictions? is there a distinction here in how we label our variables?

Residual errors is the error between the estimated value and the true/ actual value.
Error: The distance between the data point to the fitted regression line.
# Note: Remember, the line is used to find the predicted value. The actual point should be as close to that as possible.

The mean of residual errors shows how poorly the line fits with the whole dataset. This can be shown mathematically by mean square error (MSE):
MSE = 1/n * summation(n, i=1) * (yi - yhat_i)**2
# Explanation:
the difference of actual and estimated, squared, then 'meaned'/ average. Q?: Why square the difference?
Our objective is to find a line where the mean of all these errors is minimized.
The whole goal is to optimize, and in this case it means to minimize the errors between the actual values and the predicted ones (one the line).

# Q?: Is sigma notation essentially iteration?
# A!: Sigma == Summation.

The objective of linear regression is to minimize the MSE equation and to do that we must find the best parameters of theta0 and theta1, our coefficients.
# Remember, these are the parameters that we are solving for and hold our assumptions.
theta 0 == y intercept, or 'b'
theta 1 == the slope, or 'm'

How to find these to minimize error? Should we move the line a lot randomly and calculate MSE for each and find the minimum one? No.
We can use a mathematical approach, or an optimization approach.

Estimating the parameters (7'42"):

Theta 1 (the slope) is used to calculate the second parameter, the y intercept.
Theta_zero = ymean - theta_one * xmean

You must calculate teh mean of the dependent and independent data points:
theta_1 = sigma:s, i = 1: (xi - xbar)(yi - ybar) / (xi -xbar)**2
# You can reduce the squared denominator expression.

Theta_0 (ie y-intercept) can be calculated as:
theta_0 = ybar - theta_1(xbar)

Theta_0 (the y -intercept) is also known as the bias coefficient.

Predictions with linear regression (10'41"):
After we've found the parameters of the linear equation, making predictions is as simple as solving the equation for a specific set of inputs.

Pros of Linear regression:
Very fast. Basic and easy to understand
No parameter tuning (versus k in k nearest neighbors)

-------------------------------------
Model Evaluation in Regression Models
-------------------------------------

The goal of regression is to build a model to accurately predict an unknown case. We have to perform regression evaluation after building the model.

Two Approaches:
- Train and Test on teh Same Dataset
- Train/ Test Split

Best approach?
How can we trust the model we've built? We can take the entire set of data and build the model. From there, we take an eve smaller sample of the test set and use the dependent values to predict the independent. From here we then compare our predicted values, yhats, with the actual values of data should already have.
2'20"
One of the simplest methods for calculating the error of our model:
Error = 1/n Sigma(j=1, n) * abs(yj - yhat_j)
stated thus: the average of the difference of all actual values and predicted values for all rows.

This approach (train and test) is the simplest. Again, it takes the entire set and build a model with it, then uses a subset of that same set to test with.
This evaluation approach would typically have a high "training accuracy" but a low "out-of-sample" accuracy.

What is training accuracy and out of sample accuracy? 3'32"
Training Accuracy:
    - Is the percentage of correct predictions that the model makes when using the test data set
    - High training accuracy isn't necessarily a good thing.
    - Overfitting: Over trained to the data set. Non-generalized
Out-of-Sample Accuracy:
    - Percent accuracy of predictions the model has made on data outside of the training data. (Basically the numbers have been included/ accounted for in hte coefficients).
    - Our approach above (train & test) will likely have high but potentially overfitted training accuracy, and low OoSample accuracy.

It's important that the models have high OoS accuracy. How can we improve this?
One way is to use train/ test split.

4'51"
Train/ Test split evaluation approach:
A subset of the entire data set is used for the training model, and a separate subset is used for testing.
These subsets are mutually exclusive.
this will provide a more accurate OoS accuracy because the data used to test is outside of the model. This is also more realistic for real world problems.

Todo: Verify the above statements. Actually see the difference in accuracy. Is it definitvely true that this is the case? It can't be, because your data sets determine what is true.

# Note: Make sure to train the model with the testing dat afterwards, since you don't want to lose valuable data.

The issue with train test split is that it's highly dependent on the datasets on which the data was trained/ tested.

K-fold cross validation resolves most of the issues above.
How to fix a high variation that results from a dependency? Average it.
K-fold can be described this way. We break up the data into folds. Say by 25%.  For the first fold, we take the upper 25% and use it for test, adn the rest for training. This produces a prediction accuracy.
Next, we take the percnt from 26-50% and use it for test, while the rest gets used for training, etc. We then get an avg of the accuracy percentages, and I then an average of coeeficients. It is test/train split on multiple levels.

# Note: This evaluation method is outside the scope of this course, so additional research is needed.

---------------------------------------
Evaluation Metrics in Regression Models
---------------------------------------

Used to explain the performance of a model. We can compare the actual values and the predicted values to get the accuracy of a model.

Exploring a few methods, such as:
    - MAE: Mean absolute error
    - MSE: Mean squared error
    - RMSE: Root Mean Squared Error

What is an error of the model?
The measure of how far the data is from the fitted regression line. (I.e. the predicted, yhat, is the line).

Since there are multiple data points, and error can be determined in multiple ways.

MAE is the mean (average) of the difference of the errors. The easiest to understand.

MSE is the square of the difference of errors, divided by the number of errors (to get mean). This is a more popular method since it is more geared towards larger errors. This is because the squared error exponentially increases on larger errors compared to that of smaller errors.

RMSE : Is the square root of MSE. Very popular because in the same units as the response vector - the y values.
#Q?: I take this to mean that because it's not astronomical and in the same sort or range as the y values.

RAE: Relative absolute error, or residual sum of square. Uses the abs of the difference and sums.

RSE: Relative squared error. Builds upon the RAE but also squares the errors, versus abs summation only. Widely adopted by the Data Science community for calculating R**2.

R**2 is not error, necessarily, but it is a popular method for describing the accuracy of the model:
        R**2 = 1 - RSE
This represents how close the actual values are to the fitted regression line. The higher, the better.

The choice of metric depends on a number of factors, such as the data type.

------------------------------
Lab: Simple Linear Regression:
------------------------------
-- Todo: Complete for the code examples as well.

Using scikit-learn, we will get a test set of data, split on training and test, and finally use the model to predict an unknown.












---------------------------
Multiple Linear Regression:
---------------------------

There are two types of linear:
Simple is when one independent variable is used to estimate a dependent variable.
Multiple Linear Regression: Use multiple independent variables.
    -For example, using cylinders and engine size to predict CO2 levels.

Examples of multiple linear regression. There are two applications in general.
Independent variables effectiveness on prediction:
    - Ex. does revision time, gender, etc have any effect on student grades?

Predicting impacts of change:
    - Discovering how changs in the independent variables change the dependent variable

As with Simple, Multiple Linear regression is the method of predicting a continuous variable.
y is a target of multiple dependent x variables.
This type is useful because you can predict which x values are a significant predictor of the outcome variable.
Can find out how each variable impacts.

yhat = theta_0 + theta_1*x_1 + ... + theta_n * x_1
# If theta_0 is the y_intercept then its the slope of each multiplied by the x of each independent variable.

4'
Mathematically this can be shown as a vector form as well. This means it can be shown as a dot product of two vectors:
the parameters vector and the feature set vector.
y_hat = theta**T * X (# theta transposed x)

theta ** T = [theta_0, theta_1, ...]
# Theta is also referred to as the parameters.

X is the feature set (the column data), represented as

X = [1
    x_1
    x_2
    ...
    ]

# Note the first element in the feature set X is 1 so it turns the theta_0 into the intercept.

theta_transposed * X in a 1-dimensional space is a simple line.
We call anything past the first dimension a hyper plane.
We need to find optimized parameters.

Using MSE to expose the errors in the model (5'47"):
Optimizing parameters lead to a model with the fewest errors. Assuming we have already found the paremeter vecotrs- theta_transposed.
We find y_hat using he feature set now.

The residual error is the actual minus the expected.
The mean of all residual errors shows how bad the model is representing the dataset. This is shown with MSE:

MSE = 1/n * Sigma(i = 1, n) * (y_i - yhat_i)**2

This is not the only way to expose the error of a Multiple Linear Regression Model, but it is one of the most popular ways to do so.
We want to minimize the error or value of this equation. How do we do so?

Estimating multiple Linear Regression Parameters (7'49")
Q?: How to estimate theta?
 - Ordinary Least Squares: Tries to estimate the coefficients by minimizing MSE. Use the data as a matrix and Linear Algebra.
 Cons: Take a long time for matrix manipulations on large datasets.

 - Optimization Approach: An algorithm to iteratively mimimize the MSE by looking at each and finding the best coefficients.
 Gradient Descent is an option. Proper apporach if you have a large dataset.

There are more and require additional research outside the scope of this class.

(9'38")
Making predictions with multiple linear regression:
yhat = theta**T * X

From here on out its just as simple as solving for yhat with the appropriate values.
Multiple linear regression estimates the relative importance of predictors. This is shown by the slope, or the individual theta coefficients.
So, if yhat = 125 + 6.6x + 12x...
the 12x coefficeint, representing a column in the feature set, would have a higher impact on the value of yhat.

Q&A - on multiple linear regression (11'25")
Q?: How to determine when to use simple or multiple linear regression?
Q?: How many independent variables should we use for the prediction? DOes adding any increase accuracy of the model?
A!: Adding too many ind. vars without any theoretical justification can result in an over fit model.
Over-fit models are a problem in that they are not general enough for accurate predictions. To this end, it is recommended not to use too many vars.
Q?: Should independent variables be continuous?
A!: You can convert categorical data to numerical if need be. Supposing you had vehicle transmission type. You can make manual = 0, automatic = 1.
# Note: Check out the logic and the math on this. The act of converting a category to a number, and then using that number generate predictions and sole equations.

Q?: What are the linear relationships between the dependent and independent variables?
A!: Scatter plots can help show linearity. However, you may need to use a non-linear model to determine the best methods.


-------------------------------
Lab: Multiple Linear Regression
-------------------------------
Q?: How to find the y intercept in Multiple Linear regression?

Q?: sklearn.linear_model.LinearRegression().fit(x, y) - Is the kw 'fit' used to create the model? OR set the training data?
What does fit mean in this domain?































----------------------
Non-Linear Regression:
----------------------

Q?: Should we use Linear Regression?
When data shows a curvy trend, linear regression is incorrect. There may be a strong relationship between the dep and
independent vars, but just not a linear one.

Different types of regression (2'15")
Quadratic (Parabolic) regression
Cubic Regression
etc...

These are polynomial regression because the dep and indped are modelled after the nth degree polynomial in x.

What is polynomial regression (2'55")
Some curvy data can be modeled by a polynomial regression.
yhat = theta0 + theta1x + theta2x**2 + theta3x**3

A polynomial regression model can still be expressed as and transformed into a linear regression model.
For example, given the thrid degree polynomial above, we can convert/ express the x coeeficients such:
x1 = x
x2 = x**2
x3 = x**3

We can then substitute these values in the linear regression equation:
yhat = theta0 + theta1x1 + theta2x2 + theta3x3 (# The numbered x's are subscripted).
This model is now linear. This is now considered to be a special case of multiple linear regression.

Q?: Do those values then become squared/ cubed?? or no? I do not think so.
Also, you would have to know the correct line to fit in order to solve this. If in knowing that, would n't just be easier
to solve without transforming to a multiple linear regression model?

Polynomial regression models can fit linear regression using the model of Least Squares.
Least Squares: Minimizing the sum of the squares of the difference between y and yhat.

What is non-linear regression?
1. To model non-linear relationship between the dependent variable and a set of independent variables.
2. yhat must a be non-linear function of the parameters theta, no necessarily the features x
# So if x shows the relationship with yhat to be linear, we cannot assume that it's linear??
Q?: Mathematically, is linear versus non just a matter of the there being nothing to any power?
A!: (Potential) The difference depends on changes to yhats theta values, not necessarily its x values.

A model is non-linear by parameters.
In contrast we cannot use the least squares method to fit the data in non-linear regression.
In general, estimation of the parameters is not easy.

Linear versus non-linear regression (6'13"):
How to tell if a model is linear or non-linear?
    - Inspect visually (Good idea to plot the data)
    - Calculate the correlation coefficient between independent and dependent variables and, if for all vars this value is 0.7 or higher, then
    there is a linear tendency.
    - Based on accuracy

How should I model my data if it displays non-linear on a scatter plot?
    - Polynomial Regression
    - Non-linear Regression Model
    - Transform your data

--------------------------
LAB: Polynomial Regression
--------------------------

Again, polynomial regression is where the relationship between the independent variable x and the dependent variable y
is modeled as an nth degree polynomial in x. Ex:
y = b + theta_1*x + theta_2 * x**2

PolynomialFeatures() function in Scikit-learn library will generate a matrix consisting of all polynomial combinations of
the features with degree less than or equal to the specified degree. If we select degree two, we will get three features
from degrees 0, 1, and 2.

from skleanr.preprocessing import PolynomialFeatures
# Traditional split of train/ test and assigning to arrays...
poly = PolynomialFeatures(degree=2)
trainX = poly.fit_transform(train_x_array) # The training data array from dataframe.

fit_transform() takes our x values and outputs a list from power 0 to power of 2. This matrix operation then replaces our
raised power x's into a special case of multiple linear regression, which we can now solve. EX:
b + theta_1*x + theta_2 * x**2 ---> b + (theta_1 * x_1_) + (theta_2 * x_2) + ...

We can now import and use LinearRegression() to solve:
clf = linear_model.LinearRegression()
train_y = clf.fit(train_x_poly, train_y)
# Get the coefficients to view
clf.coef_
clf.intercept_

-- need to complete --


----------------------------------
LAB: Regular Non-linear Regression
----------------------------------




###################################
Week Three: Classification
###################################

------------------------------
Introduction to Classification
------------------------------
Q?: Is the union a set- so a + b - |a intersection b|? It is the two values minus any overlap?

Classification is a supervised approach.
Categorizes some unknown items into a discrete set of categories or "classes".
The target attribute is a categorical value.

Classification determines the class label for an unlaeled test case.

For example, who will end up defaulting on a loan?

Classification can be multi-class or binary.

Quiz:
What is a multi-class classifier?
A classifier that can predict a field with multiple discrete values, such as "DrugA", "DrugX", "DrugY".
# Note: It can predict multiple values, but discrete - ie they must be with the known set. We cannot predict a unknown value, or a new class value. In this example, we cannot predict a new drug that would be needed.

Application: Classification use cases
- Which category a customer belongs to
- Whether a customer churns, ie. switches to another provider.
- Whether or not a customer responds to a particular advertising campaign.

- Can use this for spam filters as well.
- Photo recognition. Speech recognition, etc.

Many problems can be expressed as the association between features and target variables.

Classification algorithms in machine learning:
- Decision Trees (ID3, C4.5, C5.0)
- Naive Bayes
- Linear Discriminant Analysis
- k-Nearest Neighbor
- Logistic Regression
- Neural Networks
- Support Vector Machines (SVM)

-------------------
K-Nearest Neighbors
-------------------

If demographic data can be used to predict group membership, the company can use this to predict and send appropriate advertising/ offers.
Our objective is to build a classifier.

Determining the class using the KNN:
Can we predict the class of an unknown by looking at it nearest neighbor? We, we can. But how accurate is our prediction? We could be wrong, and maybe it makes sense to do a majority vote over the K nearest neighbors.

What is K-Nearest Neighbor (KNN)? (3'11")
Is a classification algorithm that takes a number (K) of labeled points and uses it label (ie. make predictions) on other nearby points.

- A method for classifying cases based on their similarity to other cases.
- Cases that are near each other are said to be "neighbors"
- Based on similar cases with same class labels being near each other.
The measure of the distance between two cases is the measure of their dissimilarity.

there are different ways to calculate similarity/ dissimilarity. One is Euclidean distance.

The K-Nearest Neighbors algorithm (4'06")
1. Pick a K value
2. Calculate the distance of unknown cases from all cases.
3. Select the K-observations in the training data that are "nearest" to the unknown data point.
4. Predict the response of the unknown data point using hte most popular response value from the K-nearest neighbors.

How to choose a good K? Ad how to do Number 2?

Dis(x1, x2) = sqrt(Sigma(i=0, n)(x1_i - x2_i)**2

# Difference squared summed (of each feature) and then square root of sum.
TODO: Euclidean distance.
# So the i is representative of the different columns in the feature set. If we have age and income, i1 could be age, i2 income, and so on.
sqrt(age1 - age2)**2 + (income1 - income2)**2)

TODO: We have to normalize the data to get the accurate dissimilarity measure.
Q?: What is normalizing and what does it entail?

What is the best value for KNN (6'16")
Overfitting is bad. Choosing a really high K value though can cause us to become over-generalized.

The general solution is to reserve a part of the data for testing the accuracy of your solution. Basically, we iteratively go through the test values, choosing a new K each time. We then capture the accuracy of each of these and then select the "correct" K from that value.

Computing continuous targets using KNN (8'28")

Can be used for regression. The average or median values of surrounding values are used to predict the new value.


------------------------------------
Evaluation Metrics In Classification
------------------------------------
Jaccard index
F1 score
Log loss

Jaccard index (also known as the Jaccard similarity coeficient).
This is the size of the intersection divded by the size of the union of two labeled sets:
J(y, yhat) = |y intr yhat| / |y Une yhat|
==
   |y intersection yhat|
----------------------------
|y| + |yhat| - |y intr yhat|


The higher the number, the better the accuracy.

F1-score (2'04")
Each row shows the actual labels in the test set. Columns show the predicted labels by classifier.
In our example, the top row shows category = 1, meaning hte customer will churn. In column A is what was correctly predicted as 1, and B was  predicted as zero.
The value of the first cell A1 is 6, meaning 6 were correctly predicted as class = 1. However, 9 were predicted as zero (column B), and the should have been 1 (row 1).
Thus, this leads to tvf, positives and negatives. I.e. True Negative means that it is true that something won't happen.

Precision is the measure of accuracy:
P = TP / (TP + FP)
Recall is the true positive rate:
Recall = TP / (TP + FN)

Q?: Not sure I understand the precision for the Churn = 0. The attributes FP, TN , etc have to change relative to each classifier.
A!: It comes down to understanding TP, TN, FN, FP.
So in the case of churn = 0, it means that precision is that which was correctly predicted (TP = 24), divided by that plus that which was falsely predicted to be churn = 0 (FP = 9, from B1).

F1-score is the harmonic average of the precision recall. This is a good way to show that a classifier has a good value for both recall and precision.
F1 = 2 * (prc X rec) / (prc + rec)
Q?: Is it really 2x? A!: Yes

You can then take the average of both.

Log Loss (5'55")
Sometimes the output of a classifier is the probability of class label instead of the label itself. This may ask what % this thing is class Y.

Logarithmic loss measures the accuracy of this kind of model. So if the the value is the class, but the model predicted a low probablity of this (close to a False Negative), then we would say Log Loss is high, which would be bad.

We can calculate the log loss for each row using hte log loss equation:
(y * log(yhat) + (1 - y) * log(1 - yhat))

Then taking that, we calculate the average log loss across all rows within the test set:
LogLoss = -1/n Sigma(y * log(yhat) + (1 - y) * log(1 - yhat))


--------------
Intro to Decision Trees
--------------
Q?: What is a decision tree?
A!: The basic intuition behind a decision tree is to map out all possible decision paths in the form of a tree.

How to build a decision tree?
Q? When to know when to apply these things?

Building a decision tree with the training set (1'42")
Decision trees are built by splitting the training set into distinct nodes. Where one node contains all of or most of one category of the data.

Decision trees are about testing an attribute and branching the cases based on the results of the test. We seek to find the attribute with the highest explanation, or the one that starts to sort our data by the highest amount of distinction.
Each internal node corresponds to a test, and each branch corresponds to a result of the test. And each leaf node assigns the thing in question to a class.
For example, Sex/ Gender would be a node, and the split to Male or Female would be the result of that test, and in our drug example, the correct drug for the patient would correspond to the leaf node.

Decision tree learning algorithm (3')
A decision can be constructed by considering the attributes, 1x1.
1. Choose an attribute from your dataset.
2. Calculate the significance of the attribute in splitting of data. This is to see if it's effective.
3. Split the data based on the value of the best attribute.
4. Go back to Step 1 for each branch and repeat for the rest of the attributes.


-----------------------
Building Decision Trees
-----------------------

How do you build a decision tree?
DT's are built using recursive partitioning to classify the data. The algorithm uses the most predictive data to split the data on.
Which attribute is more predictive?
We take the attribute from the features, and use it to categorize and split the dependent variables based on that.
Ex. If we are looking for Drug A or B, we check to see if breaking things on Cholesterol helps to distinguish between or or the other drug.  So if the patient has high cholesterol, can we say whether its Drug A or Drug B that primarily is used by that feature?
Q?: What constitutes sufficient evidence for splitting the data on an attribute? Is there a percentage we require?

We say something is more predictive that other other features if it helps separate things better. In fact, more predictiventess = Less impurity + Lower Entropy (less chaos).
OR, the impurity of patients in this example refers to having one set or close to one in each of the leaves.

The choice of attribute to split data is very important and it is all about purity of the leaves after the split.
Impurity of nodes is calculated by entropy of the data in hte node.
Minimizing impurity at each step.

Entropy (3'50")
Is the amount of information disorder. Measure of randomness or uncertainty. The lower the Entropy, the less unfirom the distribution, the purer the node.
If the nodes are completely homogeneous, then the entropy = 0. If equally split, then entropy = 1.

You can calculate entropy like:
Entropy = -p(A)log(p(A)) - p(B)log(p(B))
Where p is the proportion of the attribute.
Example, if A= 5 and B = 9, the equation is:
-(9/14)log(9/14) - (5/14)log(5/14) # order not mattering due to all have a minus sign in front.
# The libs will take this.

So for example, we calculate the entropy of Cholesterol as a splitter. Our two options are Normal and High, and we are looking to predict for Drug A or B.
We then check the entropy for both Normal and high.
We should go through all the attributes and calculate the entropy after the split and then choose the best attribute.

Trying another field (6'32")
Which attribute is the best, or stated another way, which field provides the most "pure" nodes for our dependent predictor variable?
Q?: We split on binary options here, Drug A and B. When more possible choices, exist, do we take hte average of the entropies for each branch, or do we simply take the absolute value of the lowest number to determine the correct choice?
A!: See below. It looks like it is the weighted sum.

In each tree do we have less entropy?

We determine the best by finding the tree with the higher Information Gain after splitting.

What is information gain? (8')
Information gain is the information that can increase the level of certainty after splitting.
Information Gain = Entropy before split - (weighted entropy after split)

As randomness decreases, the amount of certainity increases, and vice versa.

Example: If we have a total of 14 dependents, split on Drug A and B. The weight would be the number of those dep's that were split into the subsequent branch.
So if 7 (A +B) were split into this branch, we weight that with 7/14, then * the entropy value.

The higher the information gain is the correct answer.
 info_gain = entropy before - [(weight* branch entropy) - (weight * other_branch entropy)] # in our binary example.

You now go through the remaining features to calculate to information gain for each.


----------------------------
Intro to Logistic Regression
----------------------------

What is it?
What kidn of problems an be solved by logistic regression?
In what situation do we use it?

Logistic regression is a classification algorithm for categorical variables.
Using to predict based on historical data.
For example, predicting customer churn.
We can use one r more independent variables to predict.
Logistical regression is analgous to linear regression, but in this case it tries to predict a categorical/ discrete value instead of a numerical one.
# Linear regression seeks to predict a continuous value such as the price of a house for sale.
Logistic regression dependent prediction variable is binary: Anything that we can set to zero or 1. Yes, or no, etc.
# Actually, npot strictly true. You can make multi-class. We deal with binary in this class.

In logistic regression, the independent variables must be continuous. If any are categorical, they must be "dummied" to contain numeric values.

TODO: Answer the difference between the continuous or not with the other classifeir methods.
I.e. Do we need to dummy data into numerics when dealing with decision trees or KNN?

Logistic Regression Applications (2'50")

Predict the possibility of a person having a heart attack.
Predicting mortality, diseases, etc.
Purchasing a product or halting a subscription.
Predicting the probability of failure of a given process or product.
Likelihood of defaulting on a mortgage.

Notice that not only do we predict the class of each case, but the specific probability of that case belonging to said class.

When is logistic regression suitable? (4'15")
Four situations in which this method is a good candidate:

1. If your data is categorical/ binary:
2. If you need the portability of your prediction.
3. When the data is linear separable. A line is drawn, and the data is divided by it's position to the line.
4. If you need to understand the impact of a feature (i.e. the coefficients). A Theta_1 closer to zero is not very impactful, for example.

Building a model for customer churn (6'45")

X is our data set, in the space of real numbers M xN (em by en), when M is the number of features (colmuns) and N the number of records (rows) in the dataset.
y = {0,1} # What we wan tot predict.

y_hat = P(y=1|x) can predict if y is 1 given its features x.
ALso put, that the probabilty of the class being zero is 1 minus the probability of the class being 1:
P(y=0|x) = 1 - P(y=1|x)


--------------------------------------------
Logistic Regression versus Linear Regression
--------------------------------------------
Will look at linear and see why it's not always appropriate for bianry calculations.
Will also look at the sigmoid function.
Again, the goal of logistic regression is to predict the class of each state, and also the probability of each sample belonging to the class.
Linear regression works well to predict a continuous field, ie. numerical.
How about a categorical value? We can map our categories to 0 and 1. We can represent with a scatter plot. However, our y is either 0 or 1.
We can then define a threshold for our values at 0.5. This is the line that breaks up data. This rule then determine that if <.5, class = 0.

The problem is that we have no idea that the probability is correct. What is the prob. that our state really is class 0?

The problem with using linear regression (6'50"):
Linear regression always returns a number. This requires a to create a threshold. Regardless of how big or small the input ends up being (both positive or negative), the result is always ever zero or one.
We would like a smoother line to predict between 1 and 0. We need the probability of falling into our class as well.

Instead of using a function of Theta transposed X, we can use a a sigmoid function of that. This gives us the probability of a point belonging to a class.

Our equation is now:
yhat = sigmoid(Theta**Transposed X), where the RHS is actually the probability of y =1, given x, or P(y=1|x).
# Verify the substitution above.

Sigmoid function in logistic regression (9'20")
(Explanation in detail)
AKA Logistic function

= 1 / (1 + e ** - theta_transposed X)

This equation is always between zero and 1.

What is the output of the model when using hte sigmoid function?

For example, P(Churn = 1|income, age) = 0.8, where the probability of churn being 1 is based on income and age, and the value n the RHS is that probability.
Conversely, P(Churn = 0|age, income) = 1- 0.8 = 0.2

Now our job is train the model to make good predictions here. It should be a good estimate of determining the class.

The training process (12'20")
We can find theta through the training process.
# Needed for ꝺ(O**T X), where ꝺ is sigmoid.

1. Initialize theta with random values, as with most ML algorithms.
2. Calculate the model output for a customer, which is the same as
yhat = ꝺ(theta**T X)
X is the feature vector values. Theta is the confidence (or weight) that you've set in the previous step.
The output odf this equation is the prediction value- ie. the probability that the customer belongs to this class, that the state is this class.
3. Compare the output of yhat with the actual value, y, and record it as error:
error = 1 - yhat, where 1 is the actual value.
This is the error for only state in all of the training set. Thus:
4. Calculate the error for all customers. The total error is known as the models cost, and the total is calculated by the cost function. This is for hte entire model, comparing predicted and actual values.
The lower the cost, the better the model is at estimating.
5. Theta values were chosen randomly at first. Thus, change the theta to reduce the cost.
# TODO: Undersatnd the theta calcualtion here.
6. Go back to step 2, and start another iteration.
Keep doing this until the cost is low enough.

Two questions now arise:
How can we change theta to reduce cost? and When would we stop the iterations to reduce this value?
One of the most popular ways to change theta is gradient descent.

------------------------------
Logistic Regression - Training
------------------------------

General cost function
The main objective of training and logistic regression is to change the parameters of the model, so as to be the best estimation of the labels of the samples in the dataset.
How then?
We need to look at the cost function and formulate this first. We take the derivative from here.

Cost function:
Cost(yhat, y) = 1/2 * (sigmoid(thetaT X) - y)**2

The mean squared error is used here.
J(0) = i/m Sigma(i-1, m)::Cost(yhat, y)

The minus log function can help us to calculate our cost. If 1 is desirable, then -log(yhat). Or if we want zero, -log(1-yhat).

This model penalizes cases that are not class 1.
J(0) = -1/m Sigma(i-1, m):: (y**i)log(yhat**i) + (1 - y**i)log(1-yhat**i)

Minimizing the cost function of the model (5'45")
How do we find the best parameters?
    - Minimize the cost function
How do we minimize the cost function?
    - Use an optimization approach. There are many, but we use a very popular one in the form of gradient descent.
What is Gradient Descent?
    - An iterative approach to finding the minimum of a function. A technique to use the derivative of a cost function
    to change the parameter values, in order to minimize the cost

Using gradient descent to minimize the cost? (6'55")
--snip--

Training algorithm recap
1. initialize the parameters randomly.
2. Feed the cost fucntion with training set and calculate the cost.
3. Calculate the gradient of the cost function
4. Update weights with new values
5. Go back to step 2 and feed the cost function again with the new parameters. Here, we expect less error. We expect a short value of cost as we go down each step in the curve.

6. Predict the new customer X, finally


----------------------
Support Vector Machine
----------------------

SVM is a supervised algorithm that classifies cases by finding a separator.
First works by mapping data to a high-dimensional feature space.
Then a separator is estimated.
The data cannot be separated by a line in this case. It is more of a curve to capture and separate.
Transferring to a higher dimensional space can start to show a boundary, and this can be defined by a hyper-plane.

Data transformation (3'10")
Supposing we had something that was one dimensional. This is not linearly separable. However, we can map the these values and transform by say squaring it, turning the plot into a curved parabola. Now we can divide with a line.

Mapping data into a higher-dimensional space is called kernelling.
The mathematical function used to do this is called the kernell function and can be of multiple different types, such as:
linear
polynomial
RBF: radial basis function
Sigmoid
# Most of these are already implemented for us in the code libraries.

We often times try various methods to see which fit the best.

Using SVM to find the hyperplane (4'45")

These are based on finding a hpyperplane that best divides a dataset into two classes.
One thing is to look for line that best describes the largest separation, or margin from the classes.
Support vectors are those close to the line.
Hyper-planes and decision lines require their own equations.

Optimization can also be described by gradient descent.

Advantages:
    - accurate in high-dimensional spaces
    - Memory efficient due to using the support vectors.

Disadvantages:
    - Prone to over-fitting
    - No probability estimation. This is generally desirable in most Classification models.
    - Not efficient if dataset is very big.

SVM applications (7'50")
In which situations should we use?
    - Image recognition/ analysis tasks such as handwriting or image classification
    - Text category assignment
    - Detecting Spam
    - Sentiment Analysis
    - Gene Expression CLamination
    - Regression, outlier detection adn clustering





############################
Week Four:
Clustering
############################

K-Means Clustering
---------------------------
Intro to K-Means Clustering
---------------------------

Customer segmentation is the act of partitioning a dataset into groups of customers with similar attributes. Allows companies to allocate resources to a particular group of customers for marketing, for example.
We use the data to identify hwo customers or things are similar to each other.

Clustering is a very popular method to do that described above, but it is an UNsupervised method. These are partitioned to mutually exclusive groups.

What is clustering?
Means finding clusters in a dataset, unsupervised.
It is a group of objects that are similar to other objects in the cluster, and dissimilar toi data points in other clusters.

What is different between clustering and classification (besides supervised and  unsupervised).
1. We use predefined labels for the yhats- the dependent variables. Each data instance belongs  to a particualr class.
2. Unsupervised and unlabelled. We start to see that classes might exist.

Clustering applications:
- Retail/ Marketing: identify buying pattrerns for particular groups. Or recommending
- Banking: cluseters of normal trends versus fraud.
- Insurance: fraud detection.
- publication: auto-categorize, or tag news items (that are similar) #Todo check this out for news aggregation.
- medicine: characterize patient behavior/ therapies
- biology: groups genes with similar patterns

Why clustering?
- Exploraoty dataa analysis
- Summary generation: reducing the scale
- outlier detection: especially for fraud detection or noise removal
- Finding duplicates:
- Pre-processing step:

Clustering Algorithms:
Partitioned-based clustering:
    - Produces sphere like clusters.
    - Relatively efficient
    - k-Means, k-Median, Fuzzy c-Means
Hierarchical Clustering:
    - Produce trees of clusters
    - Agglomerative, Divisive
Density-based Clustering:
    - Produce arbitrary-shaped clusters. good for noise detection.
    - DBSCAN


------------------
k-Means Clustering
------------------
What is it?
A type of partitioning clustering. K-Means divides the data into non-overlapping subsets (clusters) without any cluster-internal structure
I.e. unsupervised.
Examples within a cluster are very similar
Examples across different clusters are very different

Determine the similarity or Dissimilarity:
The distances of samples from each other is used to determine the clusters.
Q?: At what point does a range flip to "dissimilar"?
K-means tries to minimize the Intra-cluster distances while maximizing the Inter-cluster differences. All of this of course in order to create separation.

K-means also attempts to divide the data into non-overlapping clusters without any cluster-internal structure.

We can use a Menkowski distance to calculate the distance between some data. This has been seen before in previous sections:
Dis(x1, x2) = sqrt[Sigma(i=0, n)(x1_i - x2_i)**2]
Also known as Euclidean distance. You can add dimensions by simply adding them, in the form of (x1_i - x2_i)**2.

How does k-means Clustering work? (3' 49")
We have to determine the value of k, which is a difficult problem.
The key concept is that K means picks a center point for each cluster, randomly.
These center points are known as centroids of the clusters.
They should be the same feature size as our feature set (Q?: What does this mean?)
Q?: Are the centroids actual points in the dataset, or something we are randomly generating from outside but nearby the set?
A!: See below

There are two approached to choose these:
1. randomly choose the points from the data set and use these as the initial means.
2. Create random points on the plot, not related to the actual data points.

k-Means clustering - calculate the distance (5'30")
After the initialization step, which was defining the centroids of each cluster, we need to assign each data point to the closest center.
This requires calculating the distance for each point from each centroid.
This requires us to create a distance matrix showing all points related to each centroid (as column).

The main objective, as stated above, is to minimize the distance of each data point from its centroid (intra), and to maximize the distance
from the other cluster centroids (inter).

In the first example on the video, our model is poor and will result in a lot of error. That is, the points chosen for k were not good, and this is
because they were chosen randomly.
Here, error is the total distance of each point from its centroid, SSE or Sum of Squared differences:

SSE = Sigma(i=1, n)::(xi - Cj)**2 # THis doenst seem right. where is the j subscript coming from?

So, how can we turn this into better clusters with less error?
In this next step we will update each data point to be the mean for data points in its cluster.
So we move the centroids. However, each time we do this we need to recalculate the distance of each point to that centroid.

k-Means is an iterative algorithm. This means we have to repeat steps until the algorithm converges.
This results in clusters with the least amount of error, i.e. the most dense clusters available.
However, it may converge to a local optimum, not necessarily the global optimum.
So, it is highly contingent on the centroids we choose in the first place. The initial clusters really set the stage.

Q?: So how do we determine the best cluster? There must be a mathematical way to do so.

The usual process to solve this problem is to run the whole process multiple times until an acceptable solution is found.


----------------
More on k-Means
----------------
A look at accuracy and Characteristics

1. Randomly placing k centroids, one for each cluster. The farther apart the clusters are placed, the better.
2. Calculate the distance of each point from each centroid. Euclidean distance is the most popular.
3. Assign each data point (object) to its closest centroid, creating a cluster.
4. Recalculate the position of k centroids. The new centroid position is created by calculating the mean of all points in the group.
5. Repeat steps 2-4, until the centroids no longer move.

How can evaluate the "goodness" of the clusters formed through k-Means.
k-Means accuracy (1'10")
External approach:
    - Compare the clusters with the ground truth, if available. This is difficult with the model being unsupervised.
Internal Approach:
    - Average the distance between data points within a cluster.

Choosing a good "k" is difficult because it is highly dependent on the shape and scale of the distribution of points within the data set.
There are several ways to do this, but one the most popular is the iterative approach, where we go through several values for k and choose the best one
based on accuracy (the closeness of points to the centroid). Mean diff.

The problem with increasing clusters is that the distance of data points from centroids will always reduce. This means that decreasing k
will always decrease the error.

The elbow point is what gets used. This is where the rate of decrease sharply shifts.
So if trending downward at a steepish slow and then things abruptly start to level out- we choose the k at that point.
This is called the elbow method.

k-Means recap
Med and Large sized databases (relatively efficient)
Produces sphere-like clusters
Needs number of clusters(k) # May be considered a drawback.


-----------------------
Hierarchical Clustering
-----------------------
Hierarchical clustering algorithms build a hierarchy of clusters where each node is a cluster consists of its daughter nodes.

There are typically two strategies to creating this:
1. Divisive
Basically top-down.
Start with all observationbs and then break down into smaller pieces.
2. Agglomerative
Bottom-up.
Each observation starts in it's own cluster, and clusters are grouped together as we go up the chain.
More popular among Data Scientists.

Rows and columns are merged as clusters are merged. We find the smallest distance, and then create a cluster from these.
From here there are a number of distance methods to use, but we can use Euclidean and move the point of the the merged clusters to the center of that merged cluster. We then use the center (which acts a mean, really) to calculate our next cluster.
We complete this until all clusters have been merged and the tree is completed.

Hierarchical clustering is typically shown as a dendrogram.
Each merge is shown by a horizontal line. The y cooridinate of this horizontal represents the similarity of these two merged clusters to each other.
We do not need to specify the number of clusters, but at some point we may need to partition disjoint clusters. This means the hierarchy needs to be cut at some point:
Ex. Although we join up all clusters in the end, some clusters are actually very far apart and we may like to keeo only things that are relatively similar
considered as a grouping. I.e. we may still want to see the differences between some merged groups.

# Check for more content here.








-------------------------------
More on Hierarchical Clustering
-------------------------------

